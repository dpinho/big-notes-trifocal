\section{Alguns Conceitos e Definicoes em Algebra Linear}
Esta seção do apêndice se destina a apresentar algumas ferramentas da álgebra linear necessárias ao entendimento de algumas seções da dissertação, funcionando como complementação dessas seções.

\subsection{A Matriz Anti-Simétrica $[{\bf v}]_\times$}
Dado um vetor ${\bf v}=(v_1,v_2,v_3)^\top$, é possível construir uma matriz anti-simétrica com as componentes de ${\bf v}$ na forma

\begin{equation*}
[{\bf v}]_\times=
\begin{bmatrix}
0&-v_3&v_2\\
v_3&0&-v_1\\
-v_2&v_1&0
\end{bmatrix},
\end{equation*}
onde a notação $[{\bf v}]_\times$ indica o produto vetorial entre um vetor qualquer ${\bf u}$ e ${\bf v}$. 

\begin{equation*}
{\bf v} \times {\bf u} = [{\bf v}]_\times {\bf u} = ({\bf v}^\top [{\bf u}]_\times)^\top
\end{equation*}

Sabemos que o produto vetorial entre um vetor e ele mesmo é sempre o vetor nulo, portanto o vetor ${\bf v}$ é o vetor nulo à direita e à esquerda de $[{\bf v}]_\times$. Desta forma, esta matriz anti-simétrica será sempre definida por seu vetor nulo.  


\subsection{O Método das Potências para Autovalores.}
Segundo citar cox, o método das potências é baseado no fato de que uma matriz $A$ possui um único autovalor dominante, ou seja, um autovalor $\lambda$ que satisfaz $\lambda > \mu$ para todos os outros autovalores $\mu$ de $A$. O método é iterativo e consiste em estipular uma estimativa inicial para o autovetor ${\bf v_k}$ e determinar o autovetor ${\bf v_{k+1}}$ em cada iteração fazendo

\begin{equation*}
{\bf v_{k+1}}=A\,{\bf v_k},
\end{equation*}
onde aproximamos um autovetor relativo a $\lambda$ quando $k\rightarrow \infty$. E encontramos um valor aproximado para $\lambda$ calculando $\begin{Vmatrix}
A\,{\bf v_k}\end{Vmatrix}$. Se a matriz não possui um único valor dominante o método pode não convergir, portanto devemos fazer uma modificação na matriz de forma a garantir a convergência. O proceimento consiste em calcular o autovalor mais próximo de um determinado $s$, de uma matriz

\begin{equation*}
A'=(A-s\,I)^{-1}
\end{equation*}
onde para quase todas as escolhas de $s$ a matriz $A'$ terá um único autovalor dominante. Sendo $\lambda'$ o autovalor dominante de $A'$, o autovalor dominante de $A$ é dado por

\begin{equation*}
\lambda=\frac{1}{\lambda'}+s
\end{equation*} 
onde $\lambda$ será o autovalor mais próximo de $s$. Este procedimento permite procurar por todos os autovalores de uma matriz como se desejaria utilizando o método de Newton-Raphson para o polinômio característico, o qual não é muito utilizado na prática.

\subsection{A Decomposição SVD.}
Uma das formas de se obter a decomposição em valores singulares de uma matriz $A$ é aplicando a decomposição em autovalores na matriz  $A^\top A$. Assim, queremos determinar a decomposição

\begin{equation}
A=U\,D\,V^\top
\end{equation} 
onde as matrizes $U$ e $V$ são ortogonais. Observe que calculando $A^\top A$ temos

\begin{equation}
\begin{array}{rcl}
A^\top A&=&V\,D\,U^\top U\,D\,V^\top\\
&=&V\,D^2 V^{-1},
\end{array}
\end{equation}
onde $V\,D^2 V^{-1}$ é uma decomposição em autovalores.

Desta forma, aplicando a decomposição em autovalores na matriz $A^\top A$, conseguimos diretamente a matriz $V^\top$ da decomposicao SVD, e basta extrair a raiz quadrada de $D$ para obtermos os valores singulares. Com uma simples inversão de matrizes podemos obter a matriz $U$:

\begin{equation}
\begin{array}{rcll}
A&=&U\,D\,V^\top &\Rightarrow\\
U&=&A\,(D\,V^\top)^{-1} &\Rightarrow\\
U&=&A\,V^{-\top}D^{-1}.
\end{array}
\end{equation}

Não são todos os problemas que envolvem a decomposição SVD que necessitam que sejam calculadas todas a matrizes. Existem problemas onde não é necessário o cálculo da matriz $U$ por exemplo. Então a complexidade computacional vai depender também daquilo que é necessário ao problema.

\subsection{A Pseudo-Inversa de $P$}
Primeiramente precisamos definir a inversa de uma matriz quadrada diagonal. Sendo $D$ essa matriz, sua inversa será

\begin{empheq}{align*}
D_{ii}^+=
\empheqlbrace{
\begin{array}{c}
0\,\,\,, \,\,\text{se}\,\,\,D_{ii}=0\\
\frac{1}{D_{ii}}\,\,,\,\,\,\text{se}\,\,\,D_{ii}\neq0.
\end{array}}
\end{empheq}


A matriz $P_{3\times4}$ tem menor quantidade de linhas do que colunas, e nesse caso devemos extender a matriz $P$ acrescentando linhas de zeros até que a mesma se torne uma matriz quadrada. Não poderemos inverter tal matriz quadrada pois com linhas de zeros ela será singular. Assim, aplicamos a decomposição SVD (detalhada em outra subseção do apêndice) e em seguida definimos a pseudo-inversa.

\begin{equation*}
P=U\,D\,V^\top
\end{equation*}
é a decomposição SVD e podemos definir a pseudo-inversa como

\begin{equation*}
P^+=V\,D^+U^\top.
\end{equation*}

De fato, podemos verificar essa definição usando $D^+$ e o fato de que as matrizes $U$ e $V$ são ortogonais (suas inversas são suas transpostas).

\begin{equation*}
\begin{array}{rcl}
P^+P&=&V\,D^+U^\top\,U\,D\,V^\top\\
&=&V\,D^+I\,D\,V^\top\\
&=&V\,D^+D\,V^\top\\
&=&V\,I\,V^\top\\
&=&V\,V^\top\\
&=&I
\end{array}
\end{equation*}

Uma das maneiras de ralizar a decomposição SVD é aplciando a decomposição em autovalores, a qual fornece a matriz $V$ e fornece quase diretamente os valores singulares. Em seuida, fica relativamente fácil obter a matriz $U$. Em outra subseção do apêndice é apresentado um método para obtenção dos autovalores.
