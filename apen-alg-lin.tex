\section{Alguns conceitos e definições de álgebra linear}\label{sec.Apen-A}
Esta seção do apêndice se destina a apresentar algumas ferramentas da álgebra linear necessárias ao entendimento de algumas seções da dissertação, funcionando como complementação dessas seções.

\subsection*{A matriz anti-simétrica $[{\bf v}]_\times$}
Dado um vetor ${\bf v}=(v_1,v_2,v_3)^\top$, é possível construir uma matriz anti-simétrica com as componentes de ${\bf v}$ na forma

\begin{equation*}
[{\bf v}]_\times=
\begin{bmatrix}
0&-v_3&v_2\\
v_3&0&-v_1\\
-v_2&v_1&0
\end{bmatrix},
\end{equation*}
onde a notação $[{\bf v}]_\times$ indica o produto vetorial entre um vetor qualquer ${\bf u}$ e ${\bf v}$. 

\begin{equation*}
{\bf v} \times {\bf u} = [{\bf v}]_\times {\bf u} = ({\bf v}^\top [{\bf u}]_\times)^\top
\end{equation*}

Sabemos que o produto vetorial entre um vetor e ele mesmo é sempre o vetor nulo, portanto o vetor ${\bf v}$ é o vetor nulo à direita e à esquerda de $[{\bf v}]_\times$. Desta forma, esta matriz anti-simétrica será sempre definida por seu vetor nulo.  

Observe ainda, que uma matriz anti-simétrica qualquer $M$ satisfaz a relação ${\bf v}^\top M\,{\bf v}=0.$ Pois,
\begin{equation*}
\begin{array}{rcl}
{\bf v}^\top M\,{\bf v}
&=&
\begin{pmatrix}
v_1&v_2&v_3
\end{pmatrix}
\begin{bmatrix}
0&m_{12}&m_{13}\\
-m_{12}&0&m_{23}\\
-m_{13}&-m_{23}&0
\end{bmatrix}
\begin{pmatrix}
v_1\\
v_2\\
v_3
\end{pmatrix}\\
&=&
\begin{pmatrix}
-v_2 m_{12}-v_3 m_{13}&v_1 m_{12}-v_3 m_{23}&v_1 m_{13}+v_2 m_{23}
\end{pmatrix}
\begin{pmatrix}
v_1\\
v_2\\
v_3
\end{pmatrix}\\
&=&
-v_1 v_2 m_{12}-v_1 v_3 m_{13}+v_1 v_2 m_{12}-v_2 v_3 m_{23}+v_1 v_3 m_{13}+v_2 v_3 m_{23}\\
&=&0.
\end{array}
\end{equation*}

\subsection*{O método das potências para autovalores}
Segundo \citep{cox-using}, o método das potências é baseado no fato de que uma matriz $A$ possui um único autovalor dominante, ou seja, um autovalor $\lambda$ que satisfaz $\begin{Vmatrix}\lambda\end{Vmatrix} > \begin{Vmatrix}\mu\end{Vmatrix}$ para todos os outros autovalores $\mu$ de $A$. Basta aplicar uma alteração no método caso a matriz não tenha um autovalor dominante. O método é iterativo e consiste em estipular uma estimativa inicial para o autovetor ${\bf v}_k$ e determinar o autovetor ${\bf v}_{k+1}$ em cada iteração fazendo

\begin{equation*}
{\bf v}_{k+1}=A\,{\bf v}_k,
\end{equation*}
onde aproximamos um autovetor relativo a $\lambda$ quando $k\rightarrow \infty$. Encontrada a aproximação para o autovetor podemos determinar o autovalor dominante correspondente aplicando o \textit{quociente de Rayleigh}, pois sendo ${\bf v}$ o autovetor relativo a $\lambda$ temos

\begin{equation*}
A\,{\bf v}=\lambda\,{\bf v},
\end{equation*}
e aplicando o produto escalar na equação

\begin{equation*}
\begin{array}{rcl}
A\,{\bf v}&=&\lambda\,{\bf v}\\
(A\,{\bf v})\cdot{\bf v}&=&\lambda\,({\bf v}\cdot{\bf v})\\
\lambda&=&\frac{(A\,{\bf v})\cdot{\bf v}}{{\bf v}\cdot{\bf v}}.
\end{array}
\end{equation*}

Para evitar problemas de overflow/underflow podemos alterar a escala do vetor, calculando a norma do vetor ou dividindo o vetor pela maior de suas componentes em cada iteração. Sempre que dividirmos o vetor pela maior de suas componentes todas as componentes vão se manter entre $-1$ e $1$. Optando pela norma basta computar $\begin{Vmatrix}A\,{\bf v}_k\end{Vmatrix}$ a cada iteração. 


Se a matriz não possui um único autovalor dominante o método pode não convergir, ou podemos precisar determinar os demais autovalores. Nesses casos, devemos fazer uma modificação na matriz de forma a garantir a convergência. O procedimento consiste em calcular o autovalor mais próximo de um $s$ arbitrário, de uma matriz

\begin{equation*}
A'=(A-s\,I)^{-1}
\end{equation*}
onde para quase todas as escolhas de $s$ a matriz $A'$ terá um único autovalor dominante. Sendo $\lambda'$ o autovalor dominante de $A'$, o autovalor dominante de $A$ é dado por

\begin{equation*}
\lambda=\frac{1}{\lambda'}+s
\end{equation*} 
onde $\lambda$ será o autovalor mais próximo de $s$. Este procedimento permite procurar por todos os autovalores de uma matriz, dada uma boa estimativa para $s$, como se desejaria fazer utilizando o método de Newton-Raphson para o polinômio característico, o qual não é muito utilizado na prática.

\subsection*{A decomposição SVD}
Uma das formas de se obter a decomposição em valores singulares de uma matriz $A$ é aplicando a decomposição em autovalores na matriz  $A^\top A$. Assim, queremos determinar a decomposição

\begin{equation*}
A=U\,D\,V^\top
\end{equation*} 
onde as matrizes $U$ e $V$ são ortogonais. Observe que calculando $A^\top A$ temos

\begin{equation*}
\begin{array}{rcl}
A^\top A&=&V\,D\,U^\top U\,D\,V^\top\\
&=&V\,D^2 V^{-1},
\end{array}
\end{equation*}
onde $V\,D^2 V^{-1}$ é uma decomposição em autovalores.

Desta forma, aplicando a decomposição em autovalores na matriz $A^\top A$, conseguimos diretamente a matriz $V^\top$ da decomposição SVD, e basta extrair a raiz quadrada das componentes da diagonal de $D$ para obtermos os valores singulares. Com uma inversão de matrizes podemos obter a matriz $U$:

\begin{equation*}
\begin{array}{rcll}
A&=&U\,D\,V^\top &\Rightarrow\\
U&=&A\,(D\,V^\top)^{-1} &\Rightarrow\\
U&=&A\,V^{-\top}D^{-1}.
\end{array}
\end{equation*}

Não são todos os problemas que envolvem a decomposição SVD que necessitam que sejam calculadas todas a matrizes. Existem problemas onde não é necessário o cálculo da matriz $U$ por exemplo. Então a complexidade computacional vai depender também daquilo que é necessário ao problema.

\subsection*{A pseudo-inversa de $P$}
Primeiramente precisamos definir a inversa de uma matriz quadrada diagonal. Sendo $D$ essa matriz, sua inversa será

\begin{empheq}{align*}
D_{ii}^+=
\empheqlbrace{
\begin{array}{c}
0\,\,\,, \,\,\text{se}\,\,\,D_{ii}=0\\
\frac{1}{D_{ii}}\,\,,\,\,\,\text{se}\,\,\,D_{ii}\neq0.
\end{array}}
\end{empheq}


A matriz $P_{3\times4}$ tem menor quantidade de linhas do que colunas, e nesse caso devemos extender a matriz $P$ acrescentando uma linha de zeros para que a mesma se torne uma matriz quadrada. Não poderemos inverter tal matriz quadrada pois com linha de zeros ela será singular. Assim, aplicamos a decomposição SVD e em seguida definimos a pseudo-inversa.

\begin{equation*}
P=U\,D\,V^\top
\end{equation*}
é a decomposição SVD e podemos definir a pseudo-inversa como

\begin{equation*}
P^+=V\,D^+U^\top.
\end{equation*}

De fato, podemos verificar essa definição usando $D^+$ e o fato de que as matrizes $U$ e $V$ são ortogonais (suas inversas são suas transpostas).

\begin{equation*}
\begin{array}{rcl}
P^+P&=&V\,D^+U^\top\,U\,D\,V^\top\\
&=&V\,D^+I\,D\,V^\top\\
&=&V\,D^+D\,V^\top\\
&=&V\,I\,V^\top\\
&=&V\,V^\top\\
&=&I
\end{array}
\end{equation*}

Uma das maneiras de ralizar a decomposição SVD é aplciando a decomposição em autovalores, a qual fornece a matriz $V$ e fornece quase diretamente os valores singulares. Em seguida, fica relativamente fácil obter a matriz $U$. Foi apresentado anteriormente um método para obtenção dos autovalores.


